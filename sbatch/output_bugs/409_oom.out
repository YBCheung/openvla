Hello zhangy50! You are on node gpu48.  The time is Wed Apr  9 23:29:23 EEST 2025.
Wed Apr  9 23:29:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:5F:00.0 Off |                    0 |
| N/A   45C    P0             79W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
/scratch/work/zhangy50/RL/openvla
['/scratch/work/zhangy50/RL/openvla/vla-scripts', '/scratch/work/zhangy50/.conda_envs/openvla-spot/lib/python310.zip', '/scratch/work/zhangy50/.conda_envs/openvla-spot/lib/python3.10', '/scratch/work/zhangy50/.conda_envs/openvla-spot/lib/python3.10/lib-dynload', '/scratch/work/zhangy50/.conda_envs/openvla-spot/lib/python3.10/site-packages', '__editable__.openvla-0.0.3.finder.__path_hook__']
2025-04-09 23:29:59.572903: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-09 23:30:02.959482: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-04-09 23:30:02.961531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-04-09 23:30:03.293139: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-09 23:30:04.364083: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-09 23:30:10.451236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-04-09 23:30:31.101011: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2348] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
Fine-tuning OpenVLA Model `openvla/openvla-7b` on `spot_kitchen`

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:04<00:08,  4.42s/it]
Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:06<00:02,  2.75s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  1.62s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.09s/it]
trainable params: 110,828,288 || all params: 7,652,065,472 || trainable%: 1.4483
2025-04-09 23:31:28.586306: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
transforms, state shape =  (None, 8)
transforms, state:  (None, 8) action:  (None, 7) Tensor("strided_slice_5:0", shape=(None, 7), dtype=float32)
04/09 [23:31:30] INFO     | >> [*] Loading existing dataset    data_utils.py:208
                          statistics from                                       
                          /scratch/work/zhangy50/RL/spot_VLA/d                  
                          ataset/tensorflow_datasets/spot_kitc                  
                          hen/1.0.0/dataset_statistics_89c0500                  
                          3c0730b0278cfc751602212a42b6e45642b2                  
                          bc73b9a1e8c2a9a784b02.json.                           
2025-04-09 23:31:30.145483: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
transforms, state shape =  (None, 8)
transforms, state:  (None, 8) action:  (None, 7) Tensor("strided_slice_5:0", shape=(None, 7), dtype=float32)

######################################################################################
# Loading the following 1 datasets (incl. sampling weight):                         #
# spot_kitchen: ============================================================1.000000 #
######################################################################################

                 INFO     | >> [*] Threads per Dataset: [1]       dataset.py:531
                 INFO     | >> [*] Reads per Dataset: [1]         dataset.py:532
                 INFO     | >> [*] Constructing datasets...       dataset.py:535
2025-04-09 23:31:30.667511: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
transforms, state shape =  (None, 8)
transforms, state:  (None, 8) action:  (None, 7) Tensor("strided_slice_5:0", shape=(None, 7), dtype=float32)
04/09 [23:31:31] INFO     | >> [*] Applying frame transforms on   dataset.py:575
                          dataset...                                            
data_utils check <class 'dict'> <class 'dict'> False
mean <class 'numpy.ndarray'> True
04/09 [23:31:32] INFO     | >> [*] Saved dataset statistics    data_utils.py:307
                          file at path                                          
                          runs/openvla-7b+spot_kitchen+b16+lr-                  
                          2e-05+lora-r32+dropout-0.0--image_au                  
                          g/dataset_statistics.json                             
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: yibo-zhang. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /scratch/work/zhangy50/RL/openvla/wandb/run-20250409_233133-cxthtc29
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ft+openvla-7b+spot_kitchen+b16+lr-2e-05+lora-r32+dropout-0.0--image_aug
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yibo-zhang/openvla_spot
wandb: üöÄ View run at https://wandb.ai/yibo-zhang/openvla_spot/runs/cxthtc29

  0%|          | 0/20 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1744230694.553159 1049275 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: "CropAndResize" attr { key: "T" value { type: DT_FLOAT } } attr { key: "extrapolation_value" value { f: 0 } } attr { key: "method" value { s: "bilinear" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 224 } dim { size: 224 } dim { size: -7 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: "CPU" vendor: "GenuineIntel" model: "111" frequency: 2100 num_cores: 2 environment { key: "cpu_instruction_set" value: "AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2" } environment { key: "eigen" value: "3.4.90" } l1_cache_size: 49152 l2_cache_size: 2097152 l3_cache_size: 110100480 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: -8 } dim { size: -9 } dim { size: -7 } } }
W0000 00:00:1744230694.553837 1049275 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: "CropAndResize" attr { key: "T" value { type: DT_FLOAT } } attr { key: "extrapolation_value" value { f: 0 } } attr { key: "method" value { s: "bilinear" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 224 } dim { size: 224 } dim { size: -6 } } } inputs { dtype: DT_FLOAT shape { dim { size: -3 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -3 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: "CPU" vendor: "GenuineIntel" model: "111" frequency: 2100 num_cores: 2 environment { key: "cpu_instruction_set" value: "AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2" } environment { key: "eigen" value: "3.4.90" } l1_cache_size: 49152 l2_cache_size: 2097152 l3_cache_size: 110100480 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -3 } dim { size: -10 } dim { size: -11 } dim { size: -6 } } }

  5%|‚ñå         | 1/20 [00:14<04:38, 14.66s/it]
 10%|‚ñà         | 2/20 [00:15<01:56,  6.48s/it]
 15%|‚ñà‚ñå        | 3/20 [00:16<01:05,  3.86s/it]
 20%|‚ñà‚ñà        | 4/20 [00:16<00:42,  2.63s/it]
 25%|‚ñà‚ñà‚ñå       | 5/20 [00:17<00:28,  1.93s/it]
 30%|‚ñà‚ñà‚ñà       | 6/20 [00:18<00:21,  1.52s/it]Saving Model Checkpoint for Step 5
/scratch/work/zhangy50/.conda_envs/openvla-spot/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
  warnings.warn("Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.")


Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A

Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  4.04it/s][A

Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  5.08it/s][A

Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  5.64it/s][A
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  5.32it/s]
data_utils check <class 'dict'> <class 'dict'> False
mean <class 'list'> False
04/09 [23:34:00] INFO     | >> [*] Saved dataset statistics    data_utils.py:307
                          file at path                                          
                          runs/openvla-7b+spot_kitchen+b16+lr-                  
                          2e-05+lora-r32+dropout-0.0--image_au                  
                          g--5_chkpt/dataset_statistics.json                    
[2025-04-09 23:34:16,865] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -9) local_rank: 0 (pid: 1049275) of binary: /scratch/work/zhangy50/.conda_envs/openvla-spot/bin/python3.10
Traceback (most recent call last):
  File "/scratch/work/zhangy50/.conda_envs/openvla-spot/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/scratch/work/zhangy50/.conda_envs/openvla-spot/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/scratch/work/zhangy50/.conda_envs/openvla-spot/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/scratch/work/zhangy50/.conda_envs/openvla-spot/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/scratch/work/zhangy50/.conda_envs/openvla-spot/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/work/zhangy50/.conda_envs/openvla-spot/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
vla-scripts/finetune.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-09_23:34:16
  host      : gpu48.int.triton.aalto.fi
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 1049275)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 1049275
========================================================
slurmstepd: error: Detected 1 oom_kill event in StepId=6808931.batch. Some of the step tasks have been OOM Killed.



-9: out of memory

Cluster: triton
User/Group: zhangy50/zhangy50
State: OUT_OF_MEMORY (exit code 0)
Nodes: 1
Cores per node: 2
CPU Utilized: 00:01:02
CPU Efficiency: 35.23% of 00:02:56 core-walltime
Job Wall-clock time: 00:01:28
Memory Utilized: 19.97 GB
Memory Efficiency: 99.85% of 20.00 GB (20.00 GB/node)

increase cpu mem to 80G, increase dataloader buffersize to 3000